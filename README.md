## paper-notes

The idea of this repo was inspired by Jason Benn's similar [repo](https://github.com/JasonBenn/deep-learning-paper-notes/).

#### 2018-07
* Difficulty Controllable Question Generation for Reading Comprehension [[notes](papers/dc-question-generation.md)] [[link](https://arxiv.org/abs/1807.03586)]
*  Universal Transformers [[notes](papers/universal-transformers.md)] [[link](https://arxiv.org/abs/1807.03819)]
* Layer Normalization [[Notes](papers/layer-normalization.md)] [[link](https://arxiv.org/abs/1607.06450)]
* Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift [[Notes](papers/batch-normalization.md)] [[link](https://arxiv.org/abs/1502.03167)]
* Bilateral Multi-Perspective Matching for Natural Language Sentences [[Link](https://arxiv.org/abs/1702.03814)] [[Notes](papers/bilateral-matching.md)]
* Deep Contextualized Word Representations [[notes](papers/elmo.md)]
* A Model To Learn Them All [[notes](papers/model-all.md)]
* Neural Architecture Search with Reinforcement Learning. [[notes](papers/rl-search.md)]

#### 2018-08
* Asynchronous Methods for Deep Reinforcement Learning [[notes](papers/async-rl.md)]
* Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting [[notes](papers/fast-abs.md)]
* Improving Abstraction in Text Summarization [[notes](papers/improv-abs.md)]



#### 2019-02

* Learning Unsupervised Learning Rules  [[notes](papers/unsupr-rules.md)]

#### 2019-06
* Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context [[notes](papers/xl-transformer.md)]
* XLNet: Generalized Autoregressive Pretraining for Language Understanding [[notes](papers/xlnet.md)]
* COMMONSENSEQA: A Question Answering Challenge Targeting Commonsense Knowledge [[notes](papers/common-sense-qa.md)]
* AUTOSEM: Automatic Task Selection and Mixing in Multi-Task Learning [[notes](papers/autosem.md)]


#### 2020-05
* Experience Grounds Language [[link](https://arxiv.org/pdf/2004.10151.pdf)], [[Notes](papers/experience.md)]
* Proximal Policy Optimization Algorithms [[link](https://arxiv.org/pdf/1707.06347.pdf)], [[Notes](papers/ppo.md)]
* Fine-Tuning Language Models from Human Preferences [[link](https://arxiv.org/pdf/1909.08593.pdf)], [[Notes](papers/finetune-lm-rl.md)]
* Layer-wise Relevance Propagation for Neural Networks with Local Renormalization Layers [[Notes](papers/lwlr.md)]
* Distributional Reinforcement Learning for Energy-Based Sequential Models [[Notes](papers/dgp.md)]
* Calibration Of Pretrained Transformers [[link](https://arxiv.org/pdf/2003.07892.pdf)], [[Notes](papers/calib.md)]
* REALM: Retrieval-Augmented Language Model Pre-Training [[link](https://arxiv.org/pdf/2002.08909.pdf)], [[Notes](papers/realm.md)]
* Byte pair encoding is suboptimal for Language Model Pretraining [[link](https://arxiv.org/abs/2004.03720)], [[Notes](papers/bpe_sub.md.md)]
* Energy-based Models for Text [[link](https://arxiv.org/abs/2004.10188)], [[Notes](papers/ebm-text.md.md)]

#### 2020-06
* Your GAN is Secretly an Energy-based Model and You Should Use Discriminator Driven Latent Sampling [[link](https://arxiv.org/pdf/2003.06060.pdf)], [[Notes](papers/gan-ebm.md)]
* REPRESENTATION LEARNING VIA INVARIANT CAUSAL MECHANISMS [[link](https://openreview.net/pdf\?id\=9p2ekP904Rs)], [[Notes](papers/repr_inv.md)]
